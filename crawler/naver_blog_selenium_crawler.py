import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import re
import json
from time import sleep
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker
from config import OPENAI_API_KEY, KAKAO_API_KEY
from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)

# DB ì„¤ì •
DATABASE_URL = "postgresql://postgres:bassmate1119@localhost:5432/bass_ai_db"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(bind=engine)
Base = declarative_base()

# í…Œì´ë¸” ëª¨ë¸
class FishingCatch(Base):
    __tablename__ = "fishing_catches"
    id = Column(Integer, primary_key=True)
    spot_name = Column(String)
    blog_title = Column(String)
    blog_url = Column(String, unique=True)
    summary = Column(Text)
    posted_at = Column(DateTime)
    weather = Column(String)
    time_period = Column(String)
    bait_type = Column(String)
    temperature = Column(String)
    wind = Column(String)
    result = Column(Integer)

# GPT ì •ì œ
def gpt_extract_fishing_info(content: str) -> dict:
    prompt = f"""
ë„ˆëŠ” ë‚šì‹œ ì¡°í–‰ê¸°ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ **ì •í™•íˆ ì•„ë˜ JSON í˜•ì‹ë§Œ** ì¶œë ¥í•´ì•¼ í•´.
ì•„ë˜ í˜•ì‹ì—ì„œ ëˆ„ë½ëœ ê°’ì€ `null`ë¡œ ì±„ì›Œ. ì ˆëŒ€ í˜•ì‹ ë°”ê¾¸ì§€ ë§ˆ.
ê·¸ ì™¸ ì„¤ëª…, ì£¼ì„, í…ìŠ¤íŠ¸ ì—†ì´ **ì˜¤ì§ JSONë§Œ** ì¶œë ¥í•´.

ë‹¤ìŒì€ JSON í˜•ì‹ì´ë‹¤:
{{
  "date": "YYYY-MM-DD",
  "spot_name": "í¬ì¸íŠ¸ ì´ë¦„",
  "weather": "ë‚ ì”¨ ìš”ì•½",
  "time_period": "ë‚šì‹œ ì‹œê°„ëŒ€",
  "bait_type": "ì‚¬ìš© ì±„ë¹„",
  "temperature": "ê¸°ì˜¨ ë˜ëŠ” ìˆ˜ì˜¨",
  "wind": "ë°”ëŒ ì •ë³´",
  "result": 0 ë˜ëŠ” 1
}}

ë‹¤ìŒì€ ë‚šì‹œ ì¡°í–‰ê¸° ë³¸ë¬¸ì´ë‹¤:
\"\"\"{content[:1500]}\"\"\"
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë‚šì‹œ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        text = response.choices[0].message.content.strip()
        print("ğŸ’¬ GPT ì›ë³¸ ì‘ë‹µ:", text)
        match = re.search(r"\{.*\}", text, re.DOTALL)
        return json.loads(match.group()) if match else {}
    except Exception as e:
        print(f"GPT ì‹¤íŒ¨: {e}")
        return {}

def extract_field(parsed, *keys):
    for key in keys:
        if key in parsed:
            return parsed[key]
    return None

# âœ… ë‹¤ì§€ì—­ í‚¤ì›Œë“œ ê¸°ë°˜ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_blog_posts_by_region(keywords, max_pages=3):
    db = SessionLocal()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

    for keyword in keywords:
        print(f"\nğŸ—ºï¸ [ì§€ì—­ í‚¤ì›Œë“œ: {keyword}] ì‹œì‘\n")
        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
            start = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=view&query={keyword}&sm=tab_pge&start={start}"
            driver.get(url)
            sleep(2)

            links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="blog.naver.com"]')
            for link in links:
                blog_title = link.text.strip()
                blog_url = link.get_attribute("href")

                if not blog_url or "MyBlog" in blog_url:
                    continue
                if db.query(FishingCatch).filter_by(blog_url=blog_url).first():
                    continue

                try:
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[1])
                    driver.get(blog_url)
                    sleep(2)

                    iframes = driver.find_elements(By.CSS_SELECTOR, "iframe#mainFrame")
                    if iframes:
                        driver.switch_to.frame(iframes[0])

                    html = driver.page_source
                    soup = BeautifulSoup(html, 'html.parser')
                    content_div = (
                        soup.find("div", {"class": "se-main-container"}) or
                        soup.find("div", {"class": "post-view"}) or
                        soup.find("div", {"id": "post-view"}) or
                        soup.find("div", {"class": "post_ct"}) or
                        soup.find("div", {"id": "contentArea"})
                    )
                    content_text = content_div.get_text(separator="\n") if content_div else ""
                    driver.switch_to.default_content()

                    if not content_text.strip():
                        raise Exception("ë³¸ë¬¸ ì—†ìŒ")

                    parsed = gpt_extract_fishing_info(content_text)
                    spot_name = extract_field(parsed, "spot_name")
                    if not spot_name:
                        continue

                    db.add(FishingCatch(
                        spot_name=spot_name,
                        blog_title=blog_title,
                        blog_url=blog_url,
                        summary=content_text[:300],
                        posted_at=datetime.today(),
                        weather=str(extract_field(parsed, "weather")),
                        time_period=str(extract_field(parsed, "time_period")),
                        bait_type=str(extract_field(parsed, "bait_type")),
                        temperature=str(extract_field(parsed, "temperature")),
                        wind=str(extract_field(parsed, "wind")),
                        result=int(extract_field(parsed, "result") or 0)
                    ))
                    print(f"âœ… ì €ì¥ë¨: {spot_name}")

                except Exception as e:
                    print(f"âŒ ë¸”ë¡œê·¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                finally:
                    try:
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
                    except Exception:
                        break

            db.commit()
    db.close()
    driver.quit()
    print("ğŸ‰ ì „ì²´ ì§€ì—­ í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    region_keywords = [
        "ê²½ê¸°ë„ ë°°ìŠ¤ë‚šì‹œ", "ê°•ì›ë„ ë°°ìŠ¤ë‚šì‹œ", "ì¶©ì²­ë„ ë°°ìŠ¤ë‚šì‹œ",
        "ì „ë¼ë„ ë°°ìŠ¤ë‚šì‹œ", "ê²½ìƒë„ ë°°ìŠ¤ë‚šì‹œ", "ì œì£¼ë„ ë°°ìŠ¤ë‚šì‹œ"
    ]
    crawl_blog_posts_by_region(region_keywords, max_pages=10)
